xlab("Year") +
ylab("Number of Pigs Slaughtered")
accuracy(pigs_forecast, pigs)
View(pigs_forecast)
pigs_actual <- pigs %>% filter(year(Month) > 2014)
ggplot() +
geom_line(data = pigs_actual, aes(x = Month, y = Count, colour = "Actual")) +
geom_line(data = pigs_forecast, aes(x = Month, y = Count, colour = "Forecasted")) +
scale_colour_discrete(name = "Count Values") +
ggtitle("Pigs slaughtered in NSW between 2014 to 2018 (Actual vs. Predicted by SNAIVE Model)") +
xlab("Year") +
ylab("Number of Pigs Slaughtered")
accuracy(pigs_forecast, pigs)
pigs_forecast %>% autoplot(aus_livestock)
library(fpp3)
library(tidyverse)
library(dplyr)
set.seed(28789164)
#Part One
myseries <- aus_livestock %>%
filter(
Animal=="Pigs",
State=="New South Wales",
year(Month)<2015
)
myseries %>% autoplot(Count)+ggtitle("Pigs Slaughtered in NSW")+xlab("Year")+ylab("Number of Animals")
#Part 2
naivepigs <- myseries %>% model(Seasonal_Naive = SNAIVE(Count))
forecastpigs <- naivepigs %>% forecast(h= "4 years")
forecastpigs %>% autoplot(myseries)+ggtitle("Forecasted Number of Pigs Slaughtered in NSW")+xlab("Year")+ylab("Number of Animals")
#Part 3
gg_tsresiduals(naivepigs)
View(forecastpigs)
View(forecastpigs)
#Part 4
myseries2 <- aus_livestock %>%
filter(
#`Series ID` == sample(aus_livestock$`Series ID`,1),
Animal=="Pigs",
State=="New South Wales"
)
forecastpigs %>% autoplot(myseries2, level=NULL)+xlab("Year")+ylab("Number of Animals")+ggtitle("Comparison of Forecast and Real Number of Pigs Slaughtered in NSW")
accuracy(forecastpigs,myseries2)
library(fpp3)
library(tidyverse)
library(dplyr)
set.seed(28789164)
#Part One
myseries <- aus_livestock %>%
filter(
Animal=="Pigs",
State=="New South Wales",
year(Month)<2015
)
myseries %>% autoplot(Count)+ggtitle("Pigs Slaughtered in NSW")+xlab("Year")+ylab("Number of Animals")
#Part 2
naivepigs <- myseries %>% model(Seasonal_Naive = SNAIVE(Count))
forecastpigs <- naivepigs %>% forecast(h= "4 years")
forecastpigs %>% autoplot(myseries)+ggtitle("Forecasted Number of Pigs Slaughtered in NSW")+xlab("Year")+ylab("Number of Animals")
#Part 3
gg_tsresiduals(naivepigs)
library (fpp3)
pigs <- aus_livestock %>%
filter(State == "New South Wales" , Animal == "Pigs")
train <- pigs %>%
filter( year(Month) < 2015)
pigs_fit <- train %>%
model(
Mean = MEAN(Count),
Naive = NAIVE(Count),
Seasonal_naive = SNAIVE(Count),
Drift = RW(Count ~ drift())
)
pigs_fc <- pigs_fit %>%
forecast(h = 48)
fit <- train %>% model(SNAIVE(Count))
fit %>% forecast() %>% autoplot(train)
augment(fit) %>% features(.resid, ljung_box, lag=10, dof=0)
gg_tsresiduals(fit)
fit %>% forecast() %>% autoplot(train)
accuracy(pigs_fc, pigs)
View(pigs_fc)
View(train)
View(pigs_fc)
pigs_fc %>% autoplot(aus_livestock)
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(tsibbledata)
library(fabletools)
library(fable)
library(ggplot2)
library(tibble)
library(lubridate)
pig_NSW<- aus_livestock %>%
filter(Animal == "Pigs", State == "New South Wales")
pig_train <- pig_NSW %>%
filter(year(Month)<2015)
fit_pig <- pig_train %>%
model(`SNa√Øve` = SNAIVE(Count))
forecast_pig<- fit_pig %>%
forecast(h=48)
plot_pig<- forecast_pig %>%
autoplot(pig_train)+
xlab("Year Month")+
ylab("Pig Count") +
ggtitle("number of pigs slaughtered in New South Wales ")
plot_pig
library(feasts)
augment(fit_pig) %>%
features(.resid,ljung_box,lag=10,dof=2)
gg_tsresiduals(fit_pig)+
ggtitle("number of pigs slaughtered in New South Wales ")
plot_pig +
autolayer(pig_NSW,colour="red",level=NULL)+
xlab("Year Month")+
ylab("Pig Count") +
ggtitle("number of pigs slaughtered in New South Wales ")
accuracy(forecast_pig,pig_NSW)
accuracy(fit_pig)
View(forecast_pig)
knitr::opts_chunk$set(echo = TRUE)
library(fpp3)
library(tidyverse)
nswpigs_fit <- aus_livestock %>%
filter(State == "New South Wales",
Animal == "Pigs",
Month < yearmonth("2015 Jan"),
!is.na(Count)) %>%
model(Seasonal_naive = SNAIVE(Count))
nswpigs_fc <- nswpigs_fit %>%
forecast(h = "4 years")
gg_tsresiduals(nswpigs_fit)
nswpigs_fc %>%
autoplot(aus_livestock, level = NULL) +
ggtitle("Forecasts for quarterly NSW pigs slaughtered from 2015 to 2018") +
xlab("Year") + ylab("Count") +
guides(colour = guide_legend(title = "Forecast"))
nswpigs_actual <- aus_livestock %>%
filter(State == "New South Wales",
Animal == "Pigs",
!is.na(Count))
print(accuracy(nswpigs_fc, nswpigs_actual))
View(nswpigs_fc)
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
library(tidyverse)
library(fpp3)
library(tsibbledata)
# creating my training sets
aus_livestock_nsw_train <- aus_livestock %>%
filter(State == "New South Wales", Animal == "Pigs", Month <= yearmonth("2014 Dec"))
aus_livestock_nsw_train
aus_livestock_fit <- aus_livestock_nsw_train %>%
model(
Seasonal_naive = SNAIVE(Count))
aus_livestock_fit
# Generating forecast for th next 4 year
aus_livestock_fc <- aus_livestock_fit %>%
forecast(h = 48)
aus_livestock_fc
aus_livestock_fc %>%
autoplot(aus_livestock_nsw_train, level = NULL)+
ggtitle("Forecasts for monthly pigs slaughtered")+
xlab("Month") + ylab("Number of pigs slaughtered")+
guides(colour = guide_legend(title = "Pork production"))
#Checking my residuals
aus_livestock_fit %>%
gg_tsresiduals()
# plotting the forcast against actual values
aus_livestock_fit %>%
accuracy()
aus_livestock_fc %>%
accuracy(aus_livestock)
View(aus_livestock_fc)
aus_livestock_fc %>% autoplot(aus_livestock)
?sarima.sim
?sarima.Sim
library(forecast)
?sarima.Sim
?sarima.sim
?arima.sim
citation
citation(stats)
citation("stats")
library(CombMSCm)
library(CombMSC)
?sarima.sim
?sarima.Sim
setwd("/media/hhew0002/f0df6edb-45fe-4416-8076-34757a0abceb/hhew0002/Academic/Competitions/M5 Competition/M5Comp/scripts")
library(stringr)
library(dplyr)
source("./pooled_regression_base.R")
# TODO: feature scaling
horizon = 28
# sales data
all_sales_data = read.csv("../../data/sales_train_valid_truncated.csv")
# calendar data
all_calendar_data = read.csv("../../data/calendar.csv")
# add clumn for the day_of_month
all_calendar_data["day_of_month"] = as.integer(str_split_fixed(all_calendar_data$date,"-", 3)[,3])
calendar_data_train = all_calendar_data[1:(nrow(all_calendar_data)-horizon*2),]
calendar_data_test = all_calendar_data[(nrow(all_calendar_data)-horizon*2 + 1):(nrow(all_calendar_data)-horizon),]
# price data
all_price_data = read.csv("../../data/sell_prices.csv")
lag=10
output_file_name = paste("./pooled_regression_lag", lag, "_forecasts.txt", sep="")
unlink(output_file_name)
model_results = NULL
sales_data = all_sales_data[1:1000, 7:ncol(all_sales_data)]
metda_data = all_sales_data[1:1000, 1:6]
# convert sales data to matrix
sales_data <- as.matrix(sales_data)
# normalize sales data by the mean
series_means = rowMeans(sales_data, na.rm = TRUE)
sales_data_mean_normalized = sales_data/series_means
# convert sales data to list of vectors
sales_data_mean_normalized.list <- split(sales_data_mean_normalized, seq(nrow(sales_data_mean_normalized)))
# perform embedding on the list elements
embedded_sales_data.list <- lapply(sales_data_mean_normalized.list, embed, dimension = lag + 1)
no_of_rows_in_each_series = ncol(all_sales_data) - 6 - lag
meta_data_embedded = metda_data[rep(seq_len(nrow(metda_data)), each = no_of_rows_in_each_series), ]
# create one matrix and combine the metadata
embedded_sales_data = do.call(rbind, embedded_sales_data.list)
embedded_data = cbind(meta_data_embedded, embedded_sales_data)
# combine the calendar data
calendar_data_train_embedded = calendar_data_train[(lag + 1):nrow(calendar_data_train),]
embedded_data = cbind(embedded_data, calendar_data_train_embedded)
# remove the na data
embedded_data_clean = na.omit(embedded_data)
# combine the price data
final_embedded_train_data = inner_join(embedded_data_clean, all_price_data)
# create the snap variable
final_embedded_train_data$snap[final_embedded_train_data$state_id == "CA"]<-final_embedded_train_data$snap_CA
final_embedded_train_data$snap[final_embedded_train_data$state_id == "TX"]<-final_embedded_train_data$snap_TX
final_embedded_train_data$snap[final_embedded_train_data$state_id == "WI"]<-final_embedded_train_data$snap_WI
# create the series means vector
modified_no_of_rows_in_each_series = final_embedded_train_data %>% group_by(item_id, store_id) %>% count() %>% ungroup() %>% pull(n)
series_means_vector = as.numeric(rep(series_means, modified_no_of_rows_in_each_series))
# drop unwanted columns
to_drop = c("id", "wm_yr_wk", "date", "weekday", "year", "d", "snap_CA", "snap_TX", "snap_WI")
final_embedded_train_data = final_embedded_train_data[, !(names(final_embedded_train_data) %in% to_drop)]
# convert to correct data types
final_embedded_train_data$wday = factor(final_embedded_train_data$wday)
final_embedded_train_data$month = factor(final_embedded_train_data$month)
final_embedded_train_data$snap = factor(final_embedded_train_data$snap)
final_embedded_train_data$day_of_month = factor(final_embedded_train_data$day_of_month)
final_embedded_train_data$item_id = factor(final_embedded_train_data$item_id)
final_embedded_train_data$store_id = factor(final_embedded_train_data$store_id)
final_embedded_train_data$state_id = factor(final_embedded_train_data$state_id)
final_embedded_train_data$dept_id = factor(final_embedded_train_data$dept_id)
final_embedded_train_data$cat_id = factor(final_embedded_train_data$cat_id)
# fit a normal model
colnames(final_embedded_train_data)[which(names(final_embedded_train_data) == "1")] <- "y"
for (i in 2:(lag+1)){
colnames(final_embedded_train_data)[which(names(final_embedded_train_data) == toString(i))] <- paste("Lag", (i - 1), sep="")
}
final_embedded_train_data
as.data.frame(x=c(1,2,3,4), y=(5,6,7,8), z=c(9,10,11,12))
as.data.frame(x=c(1,2,3,4), y=c(5,6,7,8), z=c(9,10,11,12))
data.frame(x=c(1,2,3,4), y=c(5,6,7,8), z=c(9,10,11,12))
df = data.frame(x=c(1,2,3,4), y=c(5,6,7,8), z=c(9,10,11,12))
df$snap[$state_id == "CA"]
df = data.frame(x=c(1,2,3,4), y=c(5,6,7,8), z=c(9,10,11,12), q=c("h","h","h","l"))
df
df$snap[df$q == "h"]<-df$x
new_df = df$snap[df$q == "h"]<-df$x
new_df
df$snap <- apply(df, 1, FUN = function(x) if(x$q=="h") x elseif(x$q=="l") y)
df$snap <- apply(df, 1, FUN = function(x) if(x$q=="h") x else if(x$q=="l") y)
df$snap <- apply(df, 1, FUN = function(x) if(x[4]=="h") x else if(x[4]=="l") y)
df$snap <- apply(df, 1, FUN = function(x) if(x[4]=="h") x[1] else if(x[4]=="l") x[2])
df
final_embedded_train_data
library(stringr)
library(dplyr)
source("./pooled_regression_base.R")
# TODO: feature scaling
horizon = 28
# sales data
all_sales_data = read.csv("../../data/sales_train_valid_truncated.csv")
# calendar data
all_calendar_data = read.csv("../../data/calendar.csv")
# add clumn for the day_of_month
all_calendar_data["day_of_month"] = as.integer(str_split_fixed(all_calendar_data$date,"-", 3)[,3])
calendar_data_train = all_calendar_data[1:(nrow(all_calendar_data)-horizon*2),]
calendar_data_test = all_calendar_data[(nrow(all_calendar_data)-horizon*2 + 1):(nrow(all_calendar_data)-horizon),]
# price data
all_price_data = read.csv("../../data/sell_prices.csv")
lag=10
output_file_name = paste("./pooled_regression_lag", lag, "_forecasts.txt", sep="")
unlink(output_file_name)
model_results = NULL
sales_data = all_sales_data[1:1000, 7:ncol(all_sales_data)]
metda_data = all_sales_data[1:1000, 1:6]
# convert sales data to matrix
sales_data <- as.matrix(sales_data)
# normalize sales data by the mean
series_means = rowMeans(sales_data, na.rm = TRUE)
sales_data_mean_normalized = sales_data/series_means
# convert sales data to list of vectors
sales_data_mean_normalized.list <- split(sales_data_mean_normalized, seq(nrow(sales_data_mean_normalized)))
# perform embedding on the list elements
embedded_sales_data.list <- lapply(sales_data_mean_normalized.list, embed, dimension = lag + 1)
no_of_rows_in_each_series = ncol(all_sales_data) - 6 - lag
meta_data_embedded = metda_data[rep(seq_len(nrow(metda_data)), each = no_of_rows_in_each_series), ]
# create one matrix and combine the metadata
embedded_sales_data = do.call(rbind, embedded_sales_data.list)
embedded_data = cbind(meta_data_embedded, embedded_sales_data)
# combine the calendar data
calendar_data_train_embedded = calendar_data_train[(lag + 1):nrow(calendar_data_train),]
embedded_data = cbind(embedded_data, calendar_data_train_embedded)
# remove the na data
embedded_data_clean = na.omit(embedded_data)
# combine the price data
final_embedded_train_data = inner_join(embedded_data_clean, all_price_data)
colnames(final_embedded_train_data)
(
)
# create the snap variable
final_embedded_train_data$snap <- apply(final_embedded_train_data, 1, FUN = function(x) if(x[6]=="CA") x[29] else if(x[6]=="TX") x[30] else x[31])
?apply
final_embedded_train_data
?lapply
# combine the price data
final_embedded_train_data = inner_join(embedded_data_clean, all_price_data)
fun1 <- function(x, y, z, q) if (x == "CA") {y} else if (x == "TX") {z} else {q}
final_embedded_train_data$snap <- mapply(fun1, final_embedded_train_data$state_id, final_embedded_train_data$snap_CA, final_embedded_train_data$snap_TX, final_embedded_train_data$snap_WI)
# create the series means vector
modified_no_of_rows_in_each_series = final_embedded_train_data %>% group_by(item_id, store_id) %>% count() %>% ungroup() %>% pull(n)
series_means_vector = as.numeric(rep(series_means, modified_no_of_rows_in_each_series))
# drop unwanted columns
to_drop = c("id", "wm_yr_wk", "date", "weekday", "year", "d", "snap_CA", "snap_TX", "snap_WI")
final_embedded_train_data = final_embedded_train_data[, !(names(final_embedded_train_data) %in% to_drop)]
# convert to correct data types
final_embedded_train_data$wday = factor(final_embedded_train_data$wday)
final_embedded_train_data$month = factor(final_embedded_train_data$month)
final_embedded_train_data$snap = factor(final_embedded_train_data$snap)
final_embedded_train_data$day_of_month = factor(final_embedded_train_data$day_of_month)
final_embedded_train_data$item_id = factor(final_embedded_train_data$item_id)
final_embedded_train_data$store_id = factor(final_embedded_train_data$store_id)
final_embedded_train_data$state_id = factor(final_embedded_train_data$state_id)
final_embedded_train_data$dept_id = factor(final_embedded_train_data$dept_id)
final_embedded_train_data$cat_id = factor(final_embedded_train_data$cat_id)
# fit a normal model
colnames(final_embedded_train_data)[which(names(final_embedded_train_data) == "1")] <- "y"
for (i in 2:(lag+1)){
colnames(final_embedded_train_data)[which(names(final_embedded_train_data) == toString(i))] <- paste("Lag", (i - 1), sep="")
}
for (i in 1:(lag)){
colnames(test_data)[which(names(test_data) == toString(i))] <- paste("Lag", (i), sep="")
}
# create the final test data
test_data = final_embedded_train_data %>% group_by(item_id, store_id) %>% slice(n()) %>% ungroup()
test_data = test_data[, !(names(test_data) %in% c(paste0("Lag", lag)))]
# recursively predict for all the series until the forecast horizon
predictions = NULL
previous_week = NULL
# create the final test data
test_data = final_embedded_train_data %>% group_by(item_id, store_id) %>% slice(n()) %>% ungroup()
test_data = test_data[, !(names(test_data) %in% c(paste0("Lag", lag)))]
# recursively predict for all the series until the forecast horizon
predictions = NULL
previous_week = NULL
i = 1
print(paste0("horizon: ",i))
for (j in (lag-1):1){
name = paste("Lag", j, sep="")
colnames(test_data)[which(names(test_data) == name)] <- paste("Lag", (j+1), sep="")
}
if (i == 1){
colnames(test_data)[which(names(test_data) == "y")] <- paste("Lag", 1, sep="")
}else{
colnames(test_data)[which(names(test_data) == "new_predictions")] <- paste("Lag", 1, sep="")
}
# create the new external calendar data
external_variables = calendar_data_test[i, ]
current_week = external_variables$wm_yr_wk
# drop the previous date specific data
test_data = test_data[, !(names(test_data) %in% append(names(external_variables), c("snap")))]
# add new external calendar data into the embedded data
test_data = cbind(test_data, external_variables)
test_data$snap <- mapply(create_snap, test_data$state_id, test_data$snap_CA, test_data$snap_TX, test_data$snap_WI)
# create the snap variable
create_snap <- function(state, ca, tx, wi) if (state == "CA") {ca} else if (state == "TX") {tx} else {wi}
# create the final test data
test_data = final_embedded_train_data %>% group_by(item_id, store_id) %>% slice(n()) %>% ungroup()
test_data = test_data[, !(names(test_data) %in% c(paste0("Lag", lag)))]
# recursively predict for all the series until the forecast horizon
predictions = NULL
previous_week = NULL
print(paste0("horizon: ",i))
for (j in (lag-1):1){
name = paste("Lag", j, sep="")
colnames(test_data)[which(names(test_data) == name)] <- paste("Lag", (j+1), sep="")
}
if (i == 1){
colnames(test_data)[which(names(test_data) == "y")] <- paste("Lag", 1, sep="")
}else{
colnames(test_data)[which(names(test_data) == "new_predictions")] <- paste("Lag", 1, sep="")
}
# create the new external calendar data
external_variables = calendar_data_test[i, ]
current_week = external_variables$wm_yr_wk
# drop the previous date specific data
test_data = test_data[, !(names(test_data) %in% append(names(external_variables), c("snap")))]
# add new external calendar data into the embedded data
test_data = cbind(test_data, external_variables)
test_data$snap <- mapply(create_snap, test_data$state_id, test_data$snap_CA, test_data$snap_TX, test_data$snap_WI)
# add price data into the test data
if (is.null(previous_week) || current_week != previous_week){
test_data = test_data[, !(names(test_data) %in% c("sell_price"))]
test_data = right_join(all_price_data, test_data)
}
?with
?tapply
library(stringr)
library(dplyr)
library(parallel)
source("./pooled_regression_base.R")
# TODO: feature scaling
horizon = 28
# sales data
all_sales_data = read.csv("../data/sales_train_valid_truncated.csv")
# calendar data
all_calendar_data = read.csv("../data/calendar.csv")
# add clumn for the day_of_month
all_calendar_data["day_of_month"] = as.integer(str_split_fixed(all_calendar_data$date,"-", 3)[,3])
calendar_data_train = all_calendar_data[1:(nrow(all_calendar_data)-horizon*2),]
calendar_data_test = all_calendar_data[(nrow(all_calendar_data)-horizon*2 + 1):(nrow(all_calendar_data)-horizon),]
# price data
all_price_data = read.csv("../data/sell_prices.csv")
lag=10
output_file_name = paste("./pooled_regression_lag", lag, "_forecasts.txt", sep="")
unlink(output_file_name)
model_results = NULL
sales_data = all_sales_data[1:1000, 7:ncol(all_sales_data)]
metda_data = all_sales_data[1:1000, 1:6]
# convert sales data to matrix
sales_data <- as.matrix(sales_data)
# normalize sales data by the mean
series_means = rowMeans(sales_data, na.rm = TRUE)
sales_data_mean_normalized = sales_data/series_means
# convert sales data to list of vectors
sales_data_mean_normalized.list <- split(sales_data_mean_normalized, seq(nrow(sales_data_mean_normalized)))
# perform embedding on the list elements
embedded_sales_data.list <- mclapply(sales_data_mean_normalized.list, embed, dimension = lag + 1)
no_of_rows_in_each_series = ncol(all_sales_data) - 6 - lag
meta_data_embedded = metda_data[rep(seq_len(nrow(metda_data)), each = no_of_rows_in_each_series), ]
# create one matrix and combine the metadata
embedded_sales_data = do.call(rbind, embedded_sales_data.list)
embedded_data = cbind(meta_data_embedded, embedded_sales_data)
# combine the calendar data
calendar_data_train_embedded = calendar_data_train[(lag + 1):nrow(calendar_data_train),]
embedded_data = cbind(embedded_data, calendar_data_train_embedded)
# remove the na data
embedded_data_clean = na.omit(embedded_data)
# combine the price data
final_embedded_train_data = inner_join(embedded_data_clean, all_price_data)
library(stringr)
library(dplyr)
library(parallel)
source("./pooled_regression_base.R")
# TODO: feature scaling
horizon = 28
# sales data
all_sales_data = read.csv("../../data/sales_train_valid_truncated.csv")
# calendar data
all_calendar_data = read.csv("../../data/calendar.csv")
# add clumn for the day_of_month
all_calendar_data["day_of_month"] = as.integer(str_split_fixed(all_calendar_data$date,"-", 3)[,3])
calendar_data_train = all_calendar_data[1:(nrow(all_calendar_data)-horizon*2),]
calendar_data_test = all_calendar_data[(nrow(all_calendar_data)-horizon*2 + 1):(nrow(all_calendar_data)-horizon),]
# price data
all_price_data = read.csv("../../data/sell_prices.csv")
lag=10
output_file_name = paste("./pooled_regression_lag", lag, "_forecasts.txt", sep="")
unlink(output_file_name)
model_results = NULL
sales_data = all_sales_data[1:1000, 7:ncol(all_sales_data)]
metda_data = all_sales_data[1:1000, 1:6]
# convert sales data to matrix
sales_data <- as.matrix(sales_data)
# normalize sales data by the mean
series_means = rowMeans(sales_data, na.rm = TRUE)
sales_data_mean_normalized = sales_data/series_means
# convert sales data to list of vectors
sales_data_mean_normalized.list <- split(sales_data_mean_normalized, seq(nrow(sales_data_mean_normalized)))
# perform embedding on the list elements
embedded_sales_data.list <- mclapply(sales_data_mean_normalized.list, embed, dimension = lag + 1)
no_of_rows_in_each_series = ncol(all_sales_data) - 6 - lag
meta_data_embedded = metda_data[rep(seq_len(nrow(metda_data)), each = no_of_rows_in_each_series), ]
# create one matrix and combine the metadata
embedded_sales_data = do.call(rbind, embedded_sales_data.list)
embedded_data = cbind(meta_data_embedded, embedded_sales_data)
# combine the calendar data
calendar_data_train_embedded = calendar_data_train[(lag + 1):nrow(calendar_data_train),]
embedded_data = cbind(embedded_data, calendar_data_train_embedded)
# remove the na data
embedded_data_clean = na.omit(embedded_data)
# combine the price data
final_embedded_train_data = inner_join(embedded_data_clean, all_price_data)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, cache=TRUE)
library(fpp3)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, cache=TRUE)
library(fpp3)
us_elec_gen <- as_tsibble(fpp2::usmelec)
library(fpp3)
us_elec_gen <- as_tsibble(fpp2::usmelec)
library(fpp2)
us_elec_gen <- as_tsibble(fpp2::usmelec)
library(dplyr)
us_elec_gen <- as_tsibble(fpp2::usmelec)
